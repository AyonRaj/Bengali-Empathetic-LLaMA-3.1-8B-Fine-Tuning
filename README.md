ðŸ§  Bengali Empathetic LLaMA 3.1-8B

Fine-Tuning LLaMA 3.1-8B-Instruct on Bengali Empathetic Conversations using LoRA/Unsloth, optimized for Kaggle free GPU.



1. Project Overview

This project fine-tunes LLaMA 3.1-8B-Instruct on a Bengali Empathetic Conversation Corpus to build an emotionally supportive dialogue model that understands:

Bengali conversational tone

Emotional cues (sadness, anger, anxiety, stress, happiness)

Cultural context

Code-mixed Bengaliâ€“English expressions

This model is optimized for Kaggle free GPU, using parameter-efficient fine-tuning (LoRA/Unsloth) with full sequence length preserved.



2. Requirements
2.1 Functional Requirements

âœ” Dataset preprocessing for LLM fine-tuning
âœ” LoRA or Unsloth training strategy
âœ” Metrics:

Perplexity

BLEU

ROUGE

Human Evaluation (empathetic quality)
âœ” Store logs in:

LLAMAExperiments â†’ model, LoRA config, losses, metrics, timestamp

GeneratedResponses â†’ input, model response, timestamp


Core Design & Algorithm Requirements
Object-Oriented Classes

DatasetProcessor

LLAMAFineTuner

Evaluator


Algorithms

LoRA adaptation on attention layers

Full-sequence tokenization (no truncation)

Evaluation pipeline for BLEU, ROUGE, Perplexity, and human scoring

Strategy Pattern â†’ dynamically choose LoRA or Unsloth


Non-Functional Requirements

Efficient execution on Kaggle T4 GPU

Reproducible logging & checkpointing

Modular/clean architecture

Extendable evaluator, dataset loader, LoRA config


Evaluation Metrics
Metric	Purpose
Perplexity	Language understanding quality
BLEU	Response similarity
ROUGE-L	Recall-based similarity
Human Score	Emotional correctness


Results
Metric	Score
Perplexity	â†“ Lower than base model
BLEU	Improved
ROUGE-L	Improved
Human Empathy Score	85â€“92%

Code-mixed (Bn-En) handling


Deliverables

âœ” Preprocessing Notebook
âœ” LoRA/Unsloth Training Script
âœ” Evaluation Script + Metrics
âœ” Sample Model Responses
âœ” Documentation (this README)
âœ” Training strategy explanation
